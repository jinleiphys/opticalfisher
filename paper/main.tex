\documentclass[aps,prl,reprint,superscriptaddress,showpacs]{revtex4-2}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{xcolor}
\usepackage[colorlinks=true, breaklinks=true, linkcolor=blue, citecolor=blue, urlcolor=blue]{hyperref}

\begin{document}

\title{Scale-Invariant Information Limit in Nuclear Optical Potential Extraction}

\author{Jin Lei}
\email{jinl@tongji.edu.cn}
\affiliation{School of Physics Science and Engineering, Tongji University, Shanghai 200092, China}

\date{\today}

\begin{abstract}
How strongly can nucleon-nucleus scattering data constrain optical model potential parameters? I show that the continuous ``Igo ambiguity'' reflects a physical information limit, not insufficient data. Using Fisher information geometry with the 11-parameter Koning-Delaroche potential (including spin-orbit), I decompose the constraining power step by step: single-energy elastic scattering constrains only $D_\mathrm{eff} \approx 1.7 \pm 0.4$ parameter combinations averaged over 168 configurations; at a given energy, adding the reaction cross section $\sigma_R$ provides negligible improvement (a single datum versus 17 angular bins), while the analyzing power $A_y$ lifts $D_\mathrm{eff}$ by $\sim$30\% through spin-orbit information; combining measurements across 7 energies provides the largest improvement, raising $D_\mathrm{eff}$ to $\sim$2.2. Crucially, this limit is \emph{scale-invariant}: better experiments reduce error bars but cannot resolve the underlying parameter degeneracy, just as shorter-wavelength light improves resolution but cannot beat the diffraction limit for a given aperture. Verified across 12 nuclei ($A = 12$--$208$), 7 energies ($E = 10$--$200$~MeV), and both projectile types, these results provide the first quantitative characterization of the Igo ambiguity and establish a clear hierarchy of experimental measurements for constraining the optical potential.
\end{abstract}

\pacs{24.10.Ht, 25.40.Cm, 02.50.Tt}

\maketitle

%=====================================================
% INTRODUCTION
%=====================================================
\textit{Introduction.}---The extraction of fundamental interactions from scattering observables represents one of the quintessential inverse problems in quantum physics. Since the inception of the optical model potential (OMP) over seventy years ago~\cite{Feshbach1954,Feshbach1958,Feshbach1962}, this effective description has served as the cornerstone for nuclear reaction theory. Yet, despite the exponential growth in experimental precision and computational power, a fundamental puzzle persists: the ``inverse problem ill-posedness,'' historically known as the Igo ambiguity~\cite{Igo1958}. Distinct parameter sets, representing vastly different nuclear potentials, can yield indistinguishable scattering cross sections, creating a continuous degeneracy that defies unique determination.

Two types of ambiguity exist~\cite{Satchler1983}: \emph{discrete} ambiguity from different wavefunction node numbers (resolvable by high-energy rainbow scattering), and \emph{continuous} ambiguity where parameters trade off while maintaining the surface potential value. The qualitative origin of this continuous ambiguity is well understood: elastic scattering is primarily sensitive to the potential near the nuclear surface, making it insensitive to the detailed radial shape~\cite{Satchler1983}. However, three key questions remain unanswered: (1)~\emph{How many} independent parameter combinations can actually be constrained? (2)~Can improved experimental precision eventually resolve the degeneracy? (3)~Is the ambiguity universal across the nuclear chart?

Recent Bayesian uncertainty quantification has documented large parameter uncertainties. In a landmark comparison, King \textit{et al.}~\cite{King2019PRL} showed that Bayesian MCMC posterior distributions for optical potential parameters are strongly non-Gaussian, with only the depth $V$ and radius $r_v$ strongly correlated---in striking contrast to the frequentist $\chi^2$ approach which produced spurious correlations among all parameters. Subsequent studies confirmed these findings~\cite{Lovell2021,Catacora2019}, while principal component analysis (PCA) identified the dominant variance directions~\cite{Catacora2021}. Yet these empirical observations, while valuable, require $\sim$100,000 MCMC samples per system and lack a rigorous theoretical framework that would answer the questions above.

In this Letter, I provide the first quantitative characterization of the Igo ambiguity using Fisher information geometry. By progressively adding observables and energies, I decompose the constraining power of nucleon-nucleus scattering data, revealing that elastic scattering compresses the 11-dimensional parameter space into a drastically lower-dimensional subspace, with only $\sim$2 ``stiff'' parameter combinations constrainable while the remaining directions remain ``sloppy.''

Using Fisher information geometry, I prove that single-energy elastic scattering universally constrains only
\begin{equation}
D_\mathrm{eff} \approx 1.7 \pm 0.4
\end{equation}
independent parameter combinations out of 11 in the Koning-Delaroche potential~\cite{Koning2003} (including spin-orbit coupling). This result holds regardless of target nucleus ($A = 12$--$208$) or beam energy ($E = 10$--$200$~MeV). Crucially, this limit is \emph{scale-invariant}: improving experimental precision reduces error bars but cannot alter the ``flat'' topology of the parameter manifold. Just as increasing the pixel count cannot resolve features below the diffraction limit, reducing experimental errors cannot resolve parameter degeneracy beyond this information bound.

All calculations employ the Numerov algorithm for solving the radial Schr\"odinger equation, with finite-difference gradients validated against the coupled-channels code FRESCO~\cite{Thompson1988}. The analysis spans both spin-0 (9-parameter central potential) and spin-1/2 (11-parameter with spin-orbit) scattering, incorporating elastic cross sections, analyzing powers, and reaction and total cross sections.

%=====================================================
% METHOD
%=====================================================
\textit{Method.}---Consider nucleon-nucleus elastic scattering with the Koning-Delaroche (KD02) optical potential~\cite{Koning2003}, which has 9 parameters for the central terms: real volume $(V, r_v, a_v)$, imaginary volume $(W, r_w, a_w)$, and imaginary surface $(W_d, r_d, a_d)$. Including spin-orbit coupling with Thomas form adds 2 parameters $(V_{so}, W_{so})$ for the real and imaginary spin-orbit depths, with the spin-orbit geometry $(r_{so}, a_{so})$ fixed at the KD02 values. This yields an 11-parameter model for spin-1/2 scattering.

The question is: how much can elastic cross section data $\sigma(\theta)$ tell us about these 11 parameters? To answer this, I compute the \emph{relative sensitivity} of the cross section to each parameter:
\begin{equation}
S_i(\theta) = \frac{\partial \log \sigma(\theta)}{\partial \log p_i} = \frac{p_i}{\sigma} \frac{\partial \sigma}{\partial p_i}.
\end{equation}
Using logarithmic derivatives makes parameters with different physical units (e.g., $V \sim 50$~MeV vs $r_v \sim 1.2$~fm) directly comparable. If $S_V$ and $S_{r_v}$ point in nearly the same direction in function space, then a fractional increase in $V$ has nearly the same effect as a fractional increase in $r_v$, and the two cannot be independently determined.

This is formalized through the Fisher information matrix (FIM):
\begin{equation}
F_{ij} = \sum_\theta S_i(\theta) S_j(\theta),
\end{equation}
where the sum runs over angular bins. The FIM encodes how sensitively the cross section responds to fractional parameter changes. Diagonalizing the FIM yields eigenvalues $\{\lambda_i\}$ and corresponding eigenvectors $\{\mathbf{e}_i\}$. Each eigenvalue measures the information content along its eigenvector direction, with the fractional information $\lambda_i/\sum_j \lambda_j$. Each eigenvector $\mathbf{e}_i = (e_{i,1}, \ldots, e_{i,N_p})$ defines a linear combination of parameters; the squared component $e_{i,k}^2$ gives the fractional contribution of parameter $k$ to that mode. Large eigenvalues indicate ``stiff'' directions (well-constrained), while small eigenvalues indicate ``sloppy'' directions (poorly constrained).

The effective dimensionality counts how many stiff directions exist:
\begin{equation}
D_\mathrm{eff} = \frac{(\sum_i \lambda_i)^2}{\sum_i \lambda_i^2}.
\end{equation}
This equals $N_p$ (the number of parameters) if all directions are equally stiff, and approaches 1 if one direction dominates. Physically, $D_\mathrm{eff}$ answers: ``How many independent parameter combinations can the data actually constrain?''

Geometrically, the FIM represents the local curvature of the $\chi^2$ landscape: large eigenvalues correspond to ``stiff'' directions (well-constrained), small eigenvalues to ``sloppy'' directions (the Igo ambiguity). Finding $D_\mathrm{eff} \approx 1.7$ means the 11-dimensional parameter space collapses into a $\sim$9-dimensional degenerate manifold.

Crucially, $D_\mathrm{eff}$ is \emph{scale-invariant}. Reducing experimental error $\epsilon$ scales all eigenvalues as $\lambda_i \propto 1/\epsilon^2$, which tightens all error bars proportionally. But the \emph{ratio} of eigenvalues, and hence $D_\mathrm{eff}$, is unchanged. Better precision shrinks the allowed parameter region but cannot change its shape from a narrow valley into a sphere.

\textit{Numerical method.}---The Schr\"odinger equation for each partial wave is solved using the Numerov algorithm with matching to Coulomb functions at large radius. For spin-1/2 scattering, the solver iterates over all $(l, j)$ partial wave channels, with the spin-orbit potential entering through the Thomas form with factor of 2 ($\mathbf{l}\cdot\boldsymbol{\sigma} = 2\mathbf{l}\cdot\mathbf{s}$). From the $(l,j)$-resolved $S$-matrix, the direct and spin-flip scattering amplitudes $f(\theta)$ and $g(\theta)$ are constructed, yielding the differential cross section $d\sigma/d\Omega = |f|^2 + |g|^2$, analyzing power $A_y = 2\mathrm{Im}(fg^*) / (|f|^2 + |g|^2)$, reaction cross section $\sigma_R$, and total cross section $\sigma_T$.

Parameter sensitivities are computed using central finite differences: $\partial\sigma/\partial p_i \approx [\sigma(p_i + \delta) - \sigma(p_i - \delta)]/(2\delta)$ with $\delta = 0.01 |p_i|$. For the leading eigenvalues that determine $D_\mathrm{eff}$, this relative perturbation provides sufficient numerical accuracy; the $D_\mathrm{eff}$ values are robust to order-of-magnitude variations in $\delta$. The Numerov solver has been validated against the coupled-channels code FRESCO~\cite{Thompson1988}, achieving $S$-matrix agreement to $|\Delta S| < 0.002$ and cross-section agreement to $< 0.2\%$.

%=====================================================
% RESULTS
%=====================================================
\textit{Results.}---I systematically analyze 168 configurations: 12 nuclei ($^{12}$C to $^{208}$Pb), 7 energies (10--200~MeV), and both neutron and proton projectiles. Figure~\ref{fig:deff_universal} displays $D_\mathrm{eff}$ as a heatmap across all configurations. The remarkable uniformity ($D_\mathrm{eff} \approx 1$--$3$ everywhere) demonstrates that the information limit is universal: it depends neither on target mass nor beam energy nor projectile type. The statistics yield $D_\mathrm{eff} = 1.63 \pm 0.34$ (neutrons) and $1.73 \pm 0.52$ (protons).

\begin{figure}[t]
\centering
\includegraphics[width=\columnwidth]{fig1_deff_universal.pdf}
\caption{Universal information limit. Effective dimensionality $D_\mathrm{eff}$ across 12 nuclei and 7 energies for neutron (green) and proton (pink) projectiles. Values cluster around 1--3 regardless of system, demonstrating that the information limit is intrinsic to elastic scattering.}
\label{fig:deff_universal}
\end{figure}

Figure~\ref{fig:deff_combined} confirms this universality through systematic analysis. Panel~(a) shows no significant mass dependence from $^{12}$C to $^{208}$Pb at $E = 50$~MeV. Panel~(b) shows $D_\mathrm{eff}$ remains bounded between 1 and 3 across all energies for $n+^{40}$Ca. Panel~(c) reveals condition numbers (ratio of largest to smallest FIM eigenvalue) exceeding $10^7$, indicating an extreme hierarchy between constrained and unconstrained directions. This means the stiffest direction is ten million times more constrained than the sloppiest: constraining ``stiff'' parameters to 1\% precision requires standard experimental errors, but constraining ``sloppy'' parameters to the same level would require reducing errors by a factor of $\sqrt{10^7} \approx 3000$. For all practical purposes, the $\sim$9 ``sloppy'' dimensions are unobservable.

\begin{figure*}[t]
\centering
\includegraphics[width=\textwidth]{fig2_deff_combined.pdf}
\caption{Systematic analysis for $n+A$ scattering. (a)~$D_\mathrm{eff}$ versus mass at $E = 50$~MeV shows no $A$-dependence. (b)~$D_\mathrm{eff}$ versus energy for $n+^{40}$Ca stays bounded. (c)~Condition number exceeds $10^7$, indicating extreme hierarchy between constrained and unconstrained directions.}
\label{fig:deff_combined}
\end{figure*}

\textit{Anatomy of the information limit.}---What physics do these $\sim$2 constrained directions represent? Figure~\ref{fig:eigenvectors} shows the eigenvector composition for $n+^{40}$Ca at 50~MeV using the 11-parameter model with elastic angular distributions. The dominant eigenvector $\mathbf{e}_1$ (92\% of information) is composed primarily of the real volume radius $r_v$ (65\%) with contributions from the surface radius $r_d$ (15\%) and depth $V$ (12\%). Crucially, $V$ and $r_v$ enter $\mathbf{e}_1$ with the \emph{same sign}: both increasing together corresponds to a larger scattering potential that produces a measurably different cross section. This is the ``volume integral'' direction ($J_V \propto V r_v^3$), and it is well-constrained. The radius $r_v$ dominates over $V$ because the volume integral depends cubically on $r_v$ but only linearly on $V$: in logarithmic derivatives, $\partial\log J_V/\partial\log r_v = 3$ while $\partial\log J_V/\partial\log V = 1$, so a 1\% change in $r_v$ has three times the effect of a 1\% change in $V$. The second eigenvector $\mathbf{e}_2$ (6\% of information) mixes $r_v$ (29\%), $a_v$ (27\%), and $r_d$ (27\%), encoding the radial shape of the potential surface.

Together, these two modes capture 98\% of all information. The Igo ambiguity~\cite{Igo1958} is the \emph{orthogonal} direction: $V$ increasing while $r_v$ decreases (or vice versa) so that $J_V$ stays approximately constant. This sloppy direction has an eigenvalue $10^5$ times smaller than $\mathbf{e}_1$---the data cannot distinguish a deep, narrow potential from a shallow, wide one if both have the same scattering surface. The remaining 9 parameter directions (diffuseness, imaginary potential, and spin-orbit) contribute only 2\% combined. The imaginary and spin-orbit parameters are effectively ``invisible'' to elastic angular distributions, confirming that reaction cross-section and polarization measurements are essential for constraining them.

\begin{figure}[t]
\centering
\includegraphics[width=\columnwidth]{fig3_eigenvectors.pdf}
\caption{Eigenvector composition for elastic $n+^{40}$Ca at 50~MeV (11 parameters). The ``potential radius'' mode $\mathbf{e}_1$ (green) is dominated by $r_v$ (65\%) with $r_d$ (15\%) and $V$ (12\%); the ``radial geometry'' mode $\mathbf{e}_2$ (rose) mixes $r_v$, $a_v$, and $r_d$. Together they capture 98\% of the information.}
\label{fig:eigenvectors}
\end{figure}

Why do $r_v$ and $r_d$ appear in both eigenvectors? Both the real and imaginary potentials have Woods-Saxon shapes peaked near the nuclear surface. Because the cross section depends on the potential near the classical turning point, the real and surface radii ($r_v$, $r_d$) dominate the sensitivity while diffuseness parameters ($a_v$, $a_w$, $a_d$) produce only subtle changes in the radial tail (see End Matter, Fig.~\ref{fig:sensitivity}). The angle-resolved sensitivity curves $|S_V(\theta)|$ and $|S_{r_v}(\theta)|$ have nearly identical angular shapes, differing primarily in magnitude (End Matter, Fig.~\ref{fig:info_geometry}). This geometric collinearity is the origin of the $V$-$r_v$ degeneracy: because a fractional change in $V$ has nearly the same \emph{angular pattern} as a fractional change in $r_v$, the data cannot separate their individual contributions. Only the ``scattering surface'' combination (dominated by $r_v$ due to the cubic power in $J_V$) is measurable. But can additional observables lift this degeneracy?

\textit{Step-by-step constraint analysis.}---A key question is whether additional observables and measurements can break the degeneracy. I decompose the constraining power by progressively adding information sources, as shown in Fig.~\ref{fig:stepwise} for four representative systems.

\emph{Step 1: Elastic $d\sigma/d\Omega$ alone.}---Starting from 17 angular bins of the elastic cross section at 50~MeV, $D_\mathrm{eff} \approx 1.2$--$1.4$ for all systems studied. Only the ``potential radius'' mode (predominantly $r_v$) is constrained; the remaining 10 directions are sloppy. The eigenvalue spectrum drops by seven orders of magnitude from the stiffest to the sloppiest direction.

\emph{Step 2: Adding $\sigma_R$.}---The reaction cross section provides one additional datum sensitive to the imaginary potential. However, this single number is overwhelmed by the 17 elastic angular bins: $D_\mathrm{eff}$ increases by less than 0.001. The information from $\sigma_R$ is deposited along directions already constrained by elastic scattering, producing no meaningful improvement.

\emph{Step 3: Adding $A_y$ and $\sigma_T$.}---The analyzing power, measured at the same 17 angles, probes spin-orbit interference through the spin-flip amplitude $g(\theta)$. This provides genuinely new information: for $n+^{40}$Ca, $D_\mathrm{eff}$ rises from 1.18 to 1.52. The spin-orbit subgroup trace increases by two orders of magnitude, and the information in the imaginary subspace also grows substantially. For $n+^{120}$Sn, the improvement is even larger ($D_\mathrm{eff}: 1.16 \to 1.66$). The total cross section $\sigma_T$ provides additional constraint through the optical theorem.

\emph{Step 4: Multi-energy combination.}---This provides the largest improvement. Combining elastic $d\sigma/d\Omega$ across 7 energies (10--200~MeV) raises $D_\mathrm{eff}$ from $\sim$1.2 to $\sim$2.1--2.3, even without $A_y$ data. The combined Fisher matrix $F_\mathrm{combined} = \sum_E F(E)$ accesses the energy dependence of the potential: at low energy the real potential dominates, while at high energy absorption becomes stronger. This energy lever arm lifts the imaginary-parameter degeneracy, with the imaginary subgroup $D_\mathrm{eff}$ reaching $\sim$2.3 (compared to $\sim$1.1 at single energy). Adding all observables at all energies pushes $D_\mathrm{eff}$ to 2.2--2.5, consistent with the success of dispersive optical models~\cite{Dickhoff2019} that exploit energy-dependent dispersion relations.

\emph{Step 5: KD02 global systematics.}---Steps 1--4 all constrain the same 11 local parameters at each energy; adding data always improves constraints and raises $D_\mathrm{eff}$. Step~5 asks a \emph{different} question. The KD02 global potential~\cite{Koning2003} expresses each local parameter as a polynomial in energy, e.g., $V(E) = v_1[1 - v_2(E-E_f) + v_3(E-E_f)^2 - v_4(E-E_f)^3]$, introducing a total of 17 global coefficients. Now consider: multi-energy data determine $V$ at 7 energies ($V(10), V(20), \ldots, V(200)$~MeV). This is enough to pin down $v_1$ and $v_2$ (the overall depth and its linear energy slope), but the cubic polynomial has \emph{four} coefficients. The higher-order terms $v_3$ and $v_4$ produce differences that are far smaller than the uncertainties on $V$ itself---the data simply cannot tell a quadratic from a cubic energy dependence. The same redundancy applies to other polynomial chains ($W(E)$, $W_d(E)$, etc.).

The result is $D_\mathrm{eff} \approx 1.0$ of 17 global parameters. This does \emph{not} mean that information is lost relative to Step~4---the data still constrain the same local-parameter combinations. Rather, the global KD02 functional form has more adjustable coefficients than the data can resolve, so the additional degrees of freedom simply dilute the effective dimensionality. This has a practical implication: future global optical potentials should use more parsimonious functional forms (fewer higher-order energy terms) to avoid building in degeneracies that the data cannot break.

\begin{figure}[t]
\centering
\includegraphics[width=\columnwidth]{fig_stepwise.pdf}
\caption{Step-by-step constraint analysis for $n+^{40}$Ca. (a)~$D_\mathrm{eff}$ for 11 local parameters increases as observables and energies are added (Steps 1--4): elastic alone (1.18), $+\sigma_R$ (no change), $+A_y$ (1.52), multi-energy (2.18). Step~5 re-expresses the same data in terms of 17 KD02 global parameters: $D_\mathrm{eff} \approx 1.0$ of 17 because the polynomial energy dependence has more coefficients than the data can resolve (see text). (b)~Eigenvalue spectra at each step.}
\label{fig:stepwise}
\end{figure}

\textit{Angle-resolved sensitivity.}---The angular dependence of parameter sensitivity $|S_i(\theta)|$ (End Matter, Fig.~\ref{fig:sensitivity}) reveals that $V$ and $r_v$ have nearly identical angular patterns (explaining the Igo degeneracy), while diffuseness parameters become most distinguishable at backward angles ($\theta > 130^\circ$). The cumulative Fisher information shows that $\sim$90\% of the constraining power is captured by $\theta \approx 140^\circ$, providing practical guidance for experimental design: backward-angle measurements are disproportionately valuable for breaking parameter degeneracies.

%=====================================================
% DISCUSSION
%=====================================================
\textit{Discussion.}---The step-by-step analysis (Fig.~\ref{fig:stepwise}) reveals a clear hierarchy of experimental measurements for constraining optical potentials. At 50~MeV, elastic angular distributions alone constrain only $D_\mathrm{eff} \approx 1.2$ (the overall 168-configuration average is $1.7 \pm 0.4$); $\sigma_R$ adds negligible information due to being a single integrated quantity; $A_y$ provides genuine new information through spin-orbit interference, lifting $D_\mathrm{eff}$ by $\sim$30\%; and multi-energy data provide the largest improvement, reaching $D_\mathrm{eff} \approx 2.2$.

The scale-invariance of $D_\mathrm{eff}$ has important implications. No matter how precisely cross sections are measured, the parameter space retains its ``flat valley'' topology. This is a geometric property of how scattering encodes potential information, not a limitation of current experiments.

These results provide a rigorous foundation for the empirical findings of King \textit{et al.}~\cite{King2019PRL}, who showed using Bayesian MCMC that only $V$ and $r_v$ are strongly correlated. In the Gaussian approximation, the posterior covariance equals the inverse Fisher matrix ($\Sigma_\mathrm{post} = F^{-1}$), so $D_\mathrm{eff} \approx 1.7$ directly explains why only $\sim$2 correlation structures are visible in MCMC scatter plots. A key advantage of the Fisher approach is computational efficiency: $D_\mathrm{eff}$ requires only $2N_p + 1 = 23$ solver calls versus $\sim$100,000 MCMC samples, enabling the systematic 168-configuration scan presented here. The detailed Fisher-Bayesian comparison is given in the End Matter.

The step-by-step decomposition provides clear experimental guidance: $A_y$ measurements are more valuable than $\sigma_R$ at a single energy (17 data points sensitive to spin-orbit versus a single datum), but neither can match the constraining power of multi-energy elastic data. Dispersive optical models~\cite{Dickhoff2019} exploit precisely such energy correlations. The angle-resolved analysis (Fig.~\ref{fig:sensitivity}) shows that backward-angle measurements ($\theta > 130^\circ$) are disproportionately valuable for breaking parameter degeneracies.

One caveat: the analysis uses the Woods-Saxon parameterization. However, the information limit arises from physics---elastic scattering probes an integrated ``scattering surface'' rather than the detailed radial shape---regardless of parameterization.

%=====================================================
% CONCLUSION
%=====================================================
\textit{Conclusion.}---This work provides the first quantitative decomposition of the constraining power of nucleon-nucleus scattering data on optical potential parameters. The step-by-step analysis reveals a clear hierarchy: (1)~elastic angular distributions alone constrain only $D_\mathrm{eff} \approx 1.7 \pm 0.4$ parameter combinations out of 11, primarily the potential radii; (2)~the reaction cross section $\sigma_R$ adds negligible information; (3)~the analyzing power $A_y$ lifts $D_\mathrm{eff}$ by $\sim$30\% through spin-orbit information; (4)~multi-energy combination provides the largest improvement, reaching $D_\mathrm{eff} \approx 2.2$; and (5)~the 17-parameter KD02 global parametrization yields $D_\mathrm{eff} \approx 1$ of 17---not because information is lost, but because KD02's polynomial energy dependence (e.g., a cubic for $V(E)$) has more adjustable coefficients than the data can resolve.

Crucially, these limits are \emph{scale-invariant}: improving experimental precision reduces error bars but cannot change the topology of the degenerate manifold. The result holds universally across the nuclear chart ($A = 12$--$208$, $E = 10$--$200$~MeV, neutrons and protons).

The ``sloppy'' parameter directions do not imply that these aspects of the potential are unconstrained by all data---rather, they are insensitive to single-energy scattering measurements. The multi-energy analysis (Fig.~\ref{fig:stepwise}) demonstrates that combining data across energies probes orthogonal directions in parameter space, providing a constructive path forward. This supports the physical basis of dispersive optical models~\cite{Dickhoff2019}, which leverage the energy dependence of potentials as an additional constraint.

The angle-resolved analysis (Fig.~\ref{fig:sensitivity}) shows that backward-angle measurements ($\theta > 130^\circ$) are disproportionately valuable. For rare-isotope beam facilities where beam time is precious, the step-by-step decomposition provides actionable guidance: multi-energy angular distributions---even at moderate statistics---yield more constraining power than high-statistics measurements at a single energy, and $A_y$ measurements are more informative than $\sigma_R$. The same framework could quantify parameter degeneracy in nucleon-nucleon potentials, suggesting that ``sloppiness'' may be a universal feature of effective nuclear interactions.

%=====================================================
\begin{acknowledgments}
This work was supported by the National Natural Science Foundation of China (Grant Nos.~12475132 and 12535009) and the Fundamental Research Funds for the Central Universities.
\end{acknowledgments}

\bibliography{references}

% =====================================================
% PRL End Matter â€” placed after references in publication
% =====================================================
\onecolumngrid
\bigskip
\begin{center}
\rule{0.5\columnwidth}{0.5pt}\\[6pt]
{\large\textbf{End Matter}}\\[6pt]
\rule{0.5\columnwidth}{0.5pt}
\end{center}
\twocolumngrid

\textit{Relation to Bayesian analysis.}---The Fisher information matrix (FIM) and Bayesian posterior are connected through a fundamental identity. For a likelihood $\mathcal{L}(\mathbf{d}|\boldsymbol{\theta})$ with flat priors, the posterior covariance in the Gaussian (Laplace) approximation is
\begin{equation}
\Sigma_\mathrm{post} = F^{-1}, \quad F_{ij} = -\left\langle \frac{\partial^2 \log \mathcal{L}}{\partial \theta_i \partial \theta_j}\right\rangle.
\end{equation}
The eigenvalues $\{\lambda_i\}$ of $F$ are thus the inverse of the posterior variances along the principal axes: large $\lambda_i$ corresponds to a tightly constrained direction (small posterior variance), while small $\lambda_i$ corresponds to a sloppy direction (large posterior variance). The effective dimensionality $D_\mathrm{eff} = (\sum \lambda_i)^2 / \sum \lambda_i^2$ counts the number of directions with comparable constraint.

This framework provides a rigorous explanation for the empirical findings of King \textit{et al.}~\cite{King2019PRL}. Their Bayesian MCMC analysis of nucleon-$^{48}$Ca, $^{90}$Zr, and $^{208}$Pb elastic scattering (using a Woods-Saxon parameterization with 9 free central parameters, fixing spin-orbit) found that:
\begin{itemize}
\item Only the depth $V$ and radius $r$ of the real potential showed strong correlation in MCMC scatter plots;
\item All other parameter pairs exhibited approximately circular (uncorrelated) scatter plots;
\item The frequentist $\chi^2$ approach produced spurious strong correlations among all parameters;
\item The frequentist 95\% confidence intervals were unrealistically narrow (44--92\% empirical coverage versus the nominal 95\%).
\end{itemize}

Our Fisher analysis explains each of these observations. The FIM eigenvector decomposition (Fig.~\ref{fig:eigenvectors}) shows that $\mathbf{e}_1$ is dominated by $r_v$ (65\%) and $V$ (12\%), precisely the pair King \textit{et al.} found correlated. The remaining parameters contribute to sloppy directions with eigenvalues $10^3$--$10^7$ times smaller, rendering their correlations invisible in finite MCMC samples. The condition number exceeding $10^7$ [Fig.~\ref{fig:deff_combined}(c)] explains why the frequentist approach---which approximates the posterior as a multivariate Gaussian around the $\chi^2$ minimum---produced spurious correlations: projecting a highly anisotropic ``cigar-shaped'' posterior onto a Gaussian tangent plane creates artificial coupling between the elongated and compressed directions.

The narrow frequentist confidence intervals found in Ref.~\cite{King2019PRL} (Table~I therein) are also predicted by the Fisher analysis. The frequentist approach samples from the Gaussian approximation $\boldsymbol{\theta} \sim \mathcal{N}(\hat{\boldsymbol{\theta}}, F^{-1})$, which is valid only near the minimum. But with $D_\mathrm{eff} \approx 1.7$, the $\chi^2$ landscape has $\sim$9 nearly flat directions where the Gaussian approximation breaks down, causing the frequentist method to dramatically underestimate the true parameter uncertainty along sloppy directions.

\textit{Computational advantages.}---The Fisher approach offers several practical advantages over Bayesian MCMC for systematic studies:
\begin{enumerate}
\item \emph{Efficiency}: Computing $D_\mathrm{eff}$ for one system requires $2N_p + 1 = 23$ evaluations of the scattering solver (for $N_p = 11$), taking $\sim$20 seconds. King \textit{et al.} required $\sim$100,000 MCMC samples per system, with each sample requiring a FRESCO evaluation. The $\sim$4000-fold speedup enables the systematic scan across 168 configurations presented here---an analysis that would require $\sim$$10^7$ total solver calls with MCMC.
\item \emph{Analytical results}: The scale-invariance of $D_\mathrm{eff}$ (invariance under $F \to \alpha F$) is an exact mathematical property, whereas establishing this from MCMC would require running chains at multiple noise levels and comparing.
\item \emph{Decomposability}: The Fisher matrix can be additively decomposed by observable ($F = F_\mathrm{elastic} + F_{A_y} + F_{\sigma_R} + \cdots$) and by energy ($F = \sum_E F(E)$), enabling the step-by-step analysis in Fig.~\ref{fig:stepwise}. Bayesian posteriors cannot be similarly decomposed without re-running the full MCMC.
\item \emph{Subgroup analysis}: Block sub-matrices of $F$ quantify the information content for parameter subgroups (real volume, imaginary, spin-orbit), providing physical insight into which observables constrain which parameters.
\end{enumerate}

The two approaches are complementary. The Fisher analysis efficiently maps the global structure of parameter constraints and proves fundamental properties (scale-invariance, universality), while Bayesian MCMC captures non-Gaussian features of the posterior---such as the asymmetric tails observed by King \textit{et al.}~\cite{King2019PRL}---that become important for precise credible intervals. A natural next step is to use the FIM eigenvectors to define an optimal reparameterization that separates stiff from sloppy directions, potentially improving MCMC convergence by orders of magnitude~\cite{Catacora2021}.

\textit{KD02 global parameter analysis.}---The Koning-Delaroche potential~\cite{Koning2003} parametrizes the energy dependence of the 11 local parameters through 17 global parameters. For example, the real volume depth follows $V(E) = v_1[1 - v_2(E - E_f) + v_3(E - E_f)^2 - v_4(E - E_f)^3]$, where $E_f$ is the Fermi energy. The geometry parameters ($r_v$, $a_v$, $r_d$, $a_d$) are energy-independent.

To analyze constraints on global parameters, I construct the Jacobian matrix $J = \partial\boldsymbol{\theta}_\mathrm{local}/\partial\boldsymbol{\theta}_\mathrm{global}$, where $\boldsymbol{\theta}_\mathrm{local}$ contains all $11 \times N_E$ local parameters across $N_E$ energies. The global Fisher matrix is then $F_\mathrm{global} = J^T F_\mathrm{combined} J$, where $F_\mathrm{combined} = \mathrm{diag}(F(E_1), \ldots, F(E_{N_E}))$.

With 7 energies and all observables, $D_\mathrm{eff} \approx 1.0$ of 17 global parameters. The eigenvalue spectrum spans 15 orders of magnitude, with the largest eigenvalue corresponding to the energy-independent radius parameters. As discussed in the main text, this low $D_\mathrm{eff}$ does not reflect a loss of constraining power relative to the multi-energy local analysis ($D_\mathrm{eff} \approx 2.2$ of 11). The data still determine the same local-parameter combinations at each energy.

To understand why $D_\mathrm{eff}$ decreases when the parameter space grows, consider an analogy. Suppose a fixed ``budget'' of experimental information (say, 100 units) must be distributed among the parameters. In the 11-parameter local model, this budget is concentrated: $\sim$2 parameters each receive $\sim$45 units (well-constrained), while the remaining 9 share $\sim$10 units (sloppy); the resulting $D_\mathrm{eff} \approx 2$. Switching to the 17-parameter global model does not change the total budget, but introduces 6 additional parameters (the higher-order polynomial coefficients $v_3$, $v_4$, $d_3$, etc.) that each receive effectively zero. These ``dead'' parameters dilute $D_\mathrm{eff}$ from 2.2/11 to 1.0/17 without affecting the actual constraints on any observable.

More concretely: $V(E) = v_1[1 - v_2(E-E_f) + v_3(E-E_f)^2 - v_4(E-E_f)^3]$ has four coefficients, but elastic data at 7 energies can only pin down $v_1$ (overall depth) and $v_2$ (linear energy slope). The difference between a quadratic and cubic energy dependence is far smaller than the uncertainty on $V$ itself, so $v_3$ and $v_4$ are arbitrary. The effect is analogous to fitting a straight line with a fifth-order polynomial: the slope and intercept are well-determined, but the higher-order coefficients are unconstrained. This implies that future global optical potentials could use simpler energy-dependent functional forms without sacrificing predictive power.

\begin{figure}[htbp]
\centering
\includegraphics[width=\columnwidth]{fig4_info_geometry.pdf}
\caption{Information geometry of the Igo ambiguity. The sensitivity vectors $|S_V(\theta)|$ and $|S_{r_v}(\theta)|$ have nearly identical angular dependence, pointing in the same direction in function space. This geometric near-collinearity is the origin of the depth-radius degeneracy.}
\label{fig:info_geometry}
\end{figure}

\begin{figure}[htbp]
\centering
\includegraphics[width=\columnwidth]{fig_sensitivity.pdf}
\caption{Angle-resolved sensitivity for $n+^{40}$Ca at 50~MeV. (a)~Parameter sensitivity $|S_i(\theta)|$ showing near-identical angular patterns for $V$ and $r_v$ (Igo degeneracy); diffuseness parameters gain sensitivity at backward angles (shaded). (b)~Cumulative $D_\mathrm{eff}(\theta_\mathrm{max})$ showing that $\sim$90\% of information content is captured by $\theta \approx 140^\circ$.}
\label{fig:sensitivity}
\end{figure}

\end{document}
